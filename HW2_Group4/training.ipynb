{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the submitted model (model4.tflite) has been trained and saved as part of a hyperparameter search on another notebook. This notebook displays the exact same methodology: the same parameters, algorithms, architecture, optimizations, and order of operations. Unfortunately, running this notebook doesn't output a model able to reach the accuracy constraint. This is likely due to the randomness of the parameter initialization that is not reproducible. We are sorry for the inconvenience and can provide the hyperparameter search notebook if requested (it is called hp_finding on our group project on deepnote, Group 4). Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "6600942d8a5c4c76aa8fa2d84ac09222",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 2517,
    "execution_start": 1736118607771,
    "source_hash": "86398d8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 23:10:08.547321: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-05 23:10:08.552215: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-05 23:10:08.601672: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-05 23:10:08.602430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-05 23:10:09.514691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "Python version: OK\n",
      "TensorFlow version: OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#PREPROCESSING_TYPE = 'mel_spectrogram' # comment this line if working with MFCC\n",
    "PREPROCESSING_TYPE = 'mfcc' # comment this line if working with mel spectrogram\n",
    "\n",
    "if sys.version.split()[0] != '3.11.10':\n",
    "    print(sys.version)\n",
    "    raise RuntimeError('Wrong Python version. Go to ENVIRONMENT settings, Stop machine, Start machine')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "if tf.__version__ != '2.13.0':\n",
    "    raise RuntimeError('Wrong TF version. Go to ENVIRONMENT settings, Stop machine, Start machine')\n",
    "\n",
    "\n",
    "print('\\nPython version: OK')\n",
    "print('TensorFlow version: OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "2ed1b5416f0a4bb5b7ad711ca9e2a6b8",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 0,
    "execution_start": 1736118807243,
    "source_hash": "d4bb9e2f"
   },
   "outputs": [],
   "source": [
    "PREPROCESSING_ARGS = {\n",
    "    'sampling_rate': 16000,\n",
    "    'frame_length_in_s': 0.064,\n",
    "    'frame_step_in_s': 0.032,\n",
    "    'num_mel_bins': 100,\n",
    "    'lower_frequency': 20,\n",
    "    'upper_frequency': 1000,\n",
    "    'num_coefficients': 10  # set num_coefficients to 0 if log-Mel Spectrogram features have been used\n",
    "}\n",
    "\n",
    "N_frames = int((1 - PREPROCESSING_ARGS['frame_length_in_s']) / PREPROCESSING_ARGS['frame_step_in_s']) + 1\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.01,\n",
    "    'end_learning_rate': 1.e-5,\n",
    "    'epochs': 50,\n",
    "    'initial_sparsity': 0.0, # The 4 next args are for weight pruning\n",
    "    'final_sparsity': 0.6,\n",
    "    'begin_step_%': 0.1,\n",
    "    'end_step_%': 0.9,\n",
    "    'num_clusters': 8,\n",
    "    'width_multiplier' : 0.5,\n",
    "}\n",
    "\n",
    "if PREPROCESSING_TYPE != 'mfcc' and 'num_coefficients' in PREPROCESSING_ARGS:\n",
    "    del PREPROCESSING_ARGS['num_coefficients']\n",
    "\n",
    "LABELS = ['down', 'up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "5c752dca96bf4325bb397cc6d32006fc",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 749,
    "execution_start": 1736118610443,
    "source_hash": "9e5cdf38"
   },
   "outputs": [],
   "source": [
    "from reader import AudioReader\n",
    "from preprocessing import Padding, Normalization\n",
    "from preprocessing import MelSpectrogram\n",
    "\n",
    "class MFCC():\n",
    "    def __init__(\n",
    "        self, \n",
    "        sampling_rate,\n",
    "        frame_length_in_s,\n",
    "        frame_step_in_s,\n",
    "        num_mel_bins,\n",
    "        lower_frequency,\n",
    "        upper_frequency,\n",
    "        num_coefficients\n",
    "    ):\n",
    "\n",
    "        self.mel_spec_processor = MelSpectrogram(\n",
    "            sampling_rate, frame_length_in_s, frame_step_in_s, num_mel_bins, lower_frequency, upper_frequency\n",
    "        )\n",
    "        self.num_coefficients = num_coefficients\n",
    "\n",
    "    def get_mfccs(self, audio):\n",
    "        log_mel_spectrogram = self.mel_spec_processor.get_mel_spec(audio)\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
    "        mfccs = mfccs[..., :self.num_coefficients]\n",
    "\n",
    "        return mfccs\n",
    "\n",
    "    def get_mfccs_and_label(self, audio, label):\n",
    "        mfccs = self.get_mfccs(audio)\n",
    "\n",
    "        return mfccs, label\n",
    "\n",
    "if PREPROCESSING_TYPE == 'mfcc':\n",
    "    mfcc_processor = MFCC(**PREPROCESSING_ARGS)\n",
    "else:\n",
    "    mel_spetrogram_processor = MelSpectrogram(**PREPROCESSING_ARGS)\n",
    "\n",
    "audio_reader = AudioReader(tf.int16)\n",
    "\n",
    "padding = Padding(PREPROCESSING_ARGS['sampling_rate'])\n",
    "normalization = Normalization(tf.int16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "aecfbc579d8a4a97bdfe555ddb21e7eb",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 83,
    "execution_start": 1736118611247,
    "source_hash": "65d87221"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.list_files(['/tmp/msc-train/down*', '/tmp/msc-train/up*'])\n",
    "val_ds = tf.data.Dataset.list_files(['/tmp/msc-val/down*', '/tmp/msc-val/up*'])\n",
    "test_ds = tf.data.Dataset.list_files(['/tmp/msc-test/down*', '/tmp/msc-test/up*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "53a93b47b6444801aa3bd89145bb19b7",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 3788,
    "execution_start": 1736118611383,
    "source_hash": "930a31a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 23:10:11.827113: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 AVX512F FMA\n",
      "2025-01-05 23:10:11.829791: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n",
      "(30, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_training(feature, label):\n",
    "    feature = tf.expand_dims(feature, -1)\n",
    "    label_id = tf.argmax(label == LABELS)\n",
    "\n",
    "    return feature, label_id\n",
    "\n",
    "if PREPROCESSING_TYPE == 'mel_spectrogram':\n",
    "\n",
    "    train_ds = (train_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mel_spetrogram_processor.get_mel_spec_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(TRAINING_ARGS['batch_size']))\n",
    "    val_ds = (val_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mel_spetrogram_processor.get_mel_spec_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(TRAINING_ARGS['batch_size']))\n",
    "    test_ds = (test_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mel_spetrogram_processor.get_mel_spec_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(TRAINING_ARGS['batch_size']))\n",
    "\n",
    "else:\n",
    "\n",
    "    train_ds = (train_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mfcc_processor.get_mfccs_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(TRAINING_ARGS['batch_size']))\n",
    "    val_ds = (val_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mfcc_processor.get_mfccs_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(TRAINING_ARGS['batch_size']))\n",
    "    test_ds = (test_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mfcc_processor.get_mfccs_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(TRAINING_ARGS['batch_size']))\n",
    "        \n",
    "\n",
    "for example_batch, _ in train_ds.take(1):\n",
    "    input_shape = example_batch.shape[1:]\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "ff85201b5f2f4c05aa9d34dc8da4d69f",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 65,
    "execution_start": 1736118615227,
    "source_hash": "6ea8bb80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250105-231015\n"
     ]
    }
   ],
   "source": [
    "wm = TRAINING_ARGS['width_multiplier']\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(32 * wm), kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(64 * wm), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(64 * wm), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "f9950e974a9c4230858856726cd27179",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 605,
    "execution_start": 1736118615343,
    "source_hash": "c5bbb67d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d  (None, 14, 4, 16)         290       \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 14, 4, 16)         65        \n",
      " normalization (PruneLowMag                                      \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu   (None, 14, 4, 16)         1         \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 14, 4, 32)         9218      \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 14, 4, 32)         129       \n",
      " normalization_1 (PruneLowM                                      \n",
      " agnitude)                                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_  (None, 14, 4, 32)         1         \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 14, 4, 32)         18434     \n",
      " _2 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_  (None, 14, 4, 32)         129       \n",
      " normalization_2 (PruneLowM                                      \n",
      " agnitude)                                                       \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_  (None, 14, 4, 32)         1         \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_global  (None, 32)                1         \n",
      " _average_pooling2d (PruneL                                      \n",
      " owMagnitude)                                                    \n",
      "                                                                 \n",
      " prune_low_magnitude_dense   (None, 2)                 132       \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_softma  (None, 2)                 1         \n",
      " x (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28402 (110.99 KB)\n",
      "Trainable params: 14194 (55.45 KB)\n",
      "Non-trainable params: 14208 (55.55 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "begin_step = int(len(train_ds) * TRAINING_ARGS['epochs'] * TRAINING_ARGS['begin_step_%'])\n",
    "end_step = int(len(train_ds) * TRAINING_ARGS['epochs'] * TRAINING_ARGS['end_step_%'])\n",
    "\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=TRAINING_ARGS['initial_sparsity'],\n",
    "        final_sparsity=TRAINING_ARGS['final_sparsity'],\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "172d06ced30f43119a20d333193201b8",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 0,
    "execution_start": 1736118616000,
    "source_hash": "c82dc038"
   },
   "outputs": [],
   "source": [
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=TRAINING_ARGS['learning_rate'],\n",
    "    end_learning_rate=TRAINING_ARGS['end_learning_rate'],\n",
    "    decay_steps=len(train_ds)*TRAINING_ARGS['epochs']\n",
    ")\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(linear_decay)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='auto' # min\n",
    ")\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "39ec86370530420b9046a14ac906e528",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 135524,
    "execution_start": 1736118616055,
    "source_hash": "ab9768f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "here\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 7s 936ms/step - loss: 0.6127 - sparse_categorical_accuracy: 0.6600 - val_loss: 1.3114 - val_sparse_categorical_accuracy: 0.5000 - lr: 0.0098\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 6s 871ms/step - loss: 0.4586 - sparse_categorical_accuracy: 0.8081 - val_loss: 0.5433 - val_sparse_categorical_accuracy: 0.6850 - lr: 0.0096\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 6s 887ms/step - loss: 0.3253 - sparse_categorical_accuracy: 0.8931 - val_loss: 0.3682 - val_sparse_categorical_accuracy: 0.8500 - lr: 0.0094\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 6s 850ms/step - loss: 0.2173 - sparse_categorical_accuracy: 0.9287 - val_loss: 0.2254 - val_sparse_categorical_accuracy: 0.9200 - lr: 0.0092\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9600 - val_loss: 0.1876 - val_sparse_categorical_accuracy: 0.9250 - lr: 0.0090\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 8s 1s/step - loss: 0.1082 - sparse_categorical_accuracy: 0.9681 - val_loss: 0.2657 - val_sparse_categorical_accuracy: 0.8900 - lr: 0.0088\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 7s 1s/step - loss: 0.0776 - sparse_categorical_accuracy: 0.9756 - val_loss: 0.1328 - val_sparse_categorical_accuracy: 0.9600 - lr: 0.0086\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 6s 885ms/step - loss: 0.0648 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.0827 - val_sparse_categorical_accuracy: 0.9800 - lr: 0.0084\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 6s 875ms/step - loss: 0.0553 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.0670 - val_sparse_categorical_accuracy: 0.9800 - lr: 0.0082\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 6s 866ms/step - loss: 0.0434 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.1571 - val_sparse_categorical_accuracy: 0.9250 - lr: 0.0080\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 6s 883ms/step - loss: 0.0395 - sparse_categorical_accuracy: 0.9875 - val_loss: 0.0992 - val_sparse_categorical_accuracy: 0.9650 - lr: 0.0078\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 6s 883ms/step - loss: 0.0336 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.1361 - val_sparse_categorical_accuracy: 0.9650 - lr: 0.0076\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 6s 886ms/step - loss: 0.0428 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.0637 - val_sparse_categorical_accuracy: 0.9850 - lr: 0.0074\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 6s 870ms/step - loss: 0.0337 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.0566 - val_sparse_categorical_accuracy: 0.9700 - lr: 0.0072\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 6s 873ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1266 - val_sparse_categorical_accuracy: 0.9700 - lr: 0.0070\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 6s 886ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9944 - val_loss: 0.0740 - val_sparse_categorical_accuracy: 0.9700 - lr: 0.0068\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 6s 885ms/step - loss: 0.0240 - sparse_categorical_accuracy: 0.9931 - val_loss: 0.0669 - val_sparse_categorical_accuracy: 0.9800 - lr: 0.0066\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 6s 889ms/step - loss: 0.0240 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.1031 - val_sparse_categorical_accuracy: 0.9750 - lr: 0.0064\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 6s 886ms/step - loss: 0.0268 - sparse_categorical_accuracy: 0.9906 - val_loss: 0.1958 - val_sparse_categorical_accuracy: 0.9100 - lr: 0.0062\n",
      "Epoch 19: early stopping\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    history = model.fit(train_ds, epochs=TRAINING_ARGS['epochs'], validation_data=val_ds, callbacks=[lr_scheduler, early_stopping]) # this line cause an error only once, so we do this to not haivng the error.\n",
    "except:\n",
    "    print('here')\n",
    "    history = model.fit(train_ds, epochs=TRAINING_ARGS['epochs'], validation_data=val_ds, callbacks=[lr_scheduler, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "3108eccb03af4cddbd9ebe358eb8a83e",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 18,
    "execution_start": 1736118751627,
    "source_hash": "753b5a86"
   },
   "outputs": [],
   "source": [
    "# Strip the pruning wrappers\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model)\n",
    "model_for_export.compile(loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "aaee51c7967d4b3e8b7c88c1066a950f",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 14946,
    "execution_start": 1736118815106,
    "source_hash": "14f21df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "7/7 [==============================] - 8s 954ms/step - loss: 0.1575 - accuracy: 0.9469 - val_loss: 0.1703 - val_accuracy: 0.9400\n",
      "Epoch 2/2\n",
      "7/7 [==============================] - 6s 885ms/step - loss: 0.0906 - accuracy: 0.9688 - val_loss: 0.0924 - val_accuracy: 0.9650\n"
     ]
    }
   ],
   "source": [
    "# Apply weight clustering\n",
    "clustering_params = {\n",
    "    'number_of_clusters': TRAINING_ARGS['num_clusters'],  # Choose the number of clusters for weight clustering\n",
    "    'cluster_centroids_init': tfmot.clustering.keras.CentroidInitialization.KMEANS_PLUS_PLUS\n",
    "}\n",
    "\n",
    "model = tfmot.clustering.keras.cluster_weights(model_for_export, **clustering_params)\n",
    "\n",
    "# Compile the clustered model\n",
    "model.compile(optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the clustered model\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=2)\n",
    "model_for_export = tfmot.clustering.keras.strip_clustering(model)\n",
    "model_for_export.compile(loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "f7e3f3f3e14c444794a3c80f2b8d1b2e",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 984,
    "execution_start": 1736118830103,
    "source_hash": "aebe6dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.1078 - sparse_categorical_accuracy: 0.9325\n",
      "Training Loss: 0.0268\n",
      "Training Accuracy: 99.06%\n",
      "\n",
      "Validation Loss: 0.1958\n",
      "Validation Accuracy: 91.00%\n",
      "\n",
      "Test Loss: 0.1078\n",
      "Test Accuracy: 93.25%\n"
     ]
    }
   ],
   "source": [
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "test_loss, test_accuracy = model_for_export.evaluate(test_ds)\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "32f10a8472d1481abe798988b2599435",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 4526,
    "execution_start": 1736118831135,
    "source_hash": "a2562fad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/20250105-231015/assets\n",
      "INFO:tensorflow:Assets written to: saved_models/20250105-231015/assets\n",
      "Model size: 272.57 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "save_path = f\"saved_models/{timestamp}\"\n",
    "model_for_export.save(save_path)\n",
    "\n",
    "def get_dir_size(start_path = '.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "file_size_kb = get_dir_size(save_path) / 1024\n",
    "\n",
    "\n",
    "print(f\"Model size: {file_size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "9120a4f3c7854bb3b7bc8942ee868e77",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 2290,
    "execution_start": 1736118835715,
    "source_hash": "b1e9ed5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-05 23:13:57.154664: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-01-05 23:13:57.155452: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-01-05 23:13:57.328554: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: saved_models/20250105-231015\n",
      "2025-01-05 23:13:57.486477: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-01-05 23:13:57.486625: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: saved_models/20250105-231015\n",
      "2025-01-05 23:13:57.492862: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-01-05 23:13:57.494020: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-01-05 23:13:57.893171: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: saved_models/20250105-231015\n",
      "2025-01-05 23:13:57.910783: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 582243 microseconds.\n",
      "2025-01-05 23:13:57.928056: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "quantization_ds = tf.data.Dataset.list_files(['/tmp/msc-train/down*', '/tmp/msc-train/up*'])\n",
    "\n",
    "if PREPROCESSING_TYPE == 'mel_spectrogram':\n",
    "    quantization_ds = (quantization_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mel_spetrogram_processor.get_mel_spec_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(1))\n",
    "else:\n",
    "    quantization_ds = (quantization_ds\n",
    "                .map(audio_reader.get_audio_and_label)\n",
    "                .map(padding.pad)\n",
    "                .map(normalization.normalize)\n",
    "                .map(mfcc_processor.get_mfccs_and_label)\n",
    "                .map(prepare_for_training)\n",
    "                .batch(1))\n",
    "\n",
    "def representative_data_gen():\n",
    "    for input_value in quantization_ds.take(100):\n",
    "        yield [input_value[0]]\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(save_path)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "e9b37759a0f64dc585d406d4595ad343",
    "deepnote_cell_type": "code",
    "execution_context_id": "b7054ea3-eb09-408f-a6fe-25fbed078c9f",
    "execution_millis": 407,
    "execution_start": 1736118838055,
    "source_hash": "43918471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tflite_models/20250105-231015.tflite\n",
      "TFLite model size: 31.73 KB\n",
      "Zipped tflite size (pruned model): 12.853 KB\n"
     ]
    }
   ],
   "source": [
    "#gzip\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "tflite_path = f'tflite_models/{timestamp}.tflite'\n",
    "zip_path = f'tflite_models/{timestamp}.zip'\n",
    "print(tflite_path)\n",
    "\n",
    "with open(tflite_path, 'wb') as fp:\n",
    "    fp.write(tflite_model)\n",
    "\n",
    "file_size_kb = os.path.getsize(tflite_path) / 1024\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.writestr(timestamp, tflite_model)\n",
    "\n",
    "zipped_size = os.path.getsize(zip_path) / 1024.0\n",
    "print(f\"TFLite model size: {file_size_kb:.2f} KB\")\n",
    "print(f'Zipped tflite size (pruned model): {zipped_size:.3f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b6b1fe17-4020-4f88-867f-48004baa1058' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "9ae29f85870d4e3294a4f8cd29f39491",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
